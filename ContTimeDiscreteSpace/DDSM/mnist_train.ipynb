{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from lib.losses.losses import loss_fn\n",
    "import os\n",
    "from lib.models.networks import MNISTScoreNet\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import make_grid\n",
    "from lib.config.config_bin_mnist import get_config \n",
    "from lib.datasets.datasets import get_mnist_dataset\n",
    "from lib.sampling.sampling import Euler_Maruyama_sampler\n",
    "# Main file which contrains all DDSM logic\n",
    "from lib.models.ddsm import *\n",
    "#from lib.utils.utils import binary_to_onehot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resume = True\n",
    "train_resume_path = 'SavedModels/Bin_MNIST/'\n",
    "\n",
    "if not train_resume:\n",
    "    config = get_config()\n",
    "    bookkeeping.save_config(config, config.saving.save_location)\n",
    "\n",
    "else:\n",
    "    path = train_resume_path\n",
    "    date = \"2023-09-11\"\n",
    "    config_name = \"config_001.yaml\"\n",
    "    config_path = os.path.join(path, date, config_name)\n",
    "    config = bookkeeping.load_config(config_path)\n",
    "\n",
    "device = config.device\n",
    "\n",
    "sb = UnitStickBreakingTransform()\n",
    "v_one, v_zero, v_one_loggrad, v_zero_loggrad, timepoints = torch.load(\n",
    "    config.loading.diffusion_weights_path\n",
    ")\n",
    "v_one = v_one.cpu()\n",
    "v_zero = v_zero.cpu()\n",
    "v_one_loggrad = v_one_loggrad.cpu()\n",
    "v_zero_loggrad = v_zero_loggrad.cpu()\n",
    "timepoints = timepoints.cpu()\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "alpha = torch.ones(config.data.num_cat - 1).float()\n",
    "beta = torch.arange(config.data.num_cat - 1, 0, -1).float()\n",
    "\n",
    "if config.use_fast_diff:\n",
    "    diffuser_func = partial(\n",
    "        diffusion_fast_flatdirichlet,\n",
    "        noise_factory_one=v_one,\n",
    "        v_one_loggrad=v_one_loggrad,\n",
    "    )\n",
    "else:\n",
    "    diffuser_func = partial(\n",
    "        diffusion_factory,\n",
    "        noise_factory_one=v_one,\n",
    "        noise_factory_zero=v_zero,\n",
    "        noise_factory_one_loggrad=v_one_loggrad,\n",
    "        noise_factory_zero_loggrad=v_zero_loggrad,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        device=config.device,\n",
    "    )\n",
    "\n",
    "\n",
    "if config.speed_balanced:\n",
    "    s = 2 / (\n",
    "        torch.ones(config.data.num_cat - 1, device=config.device)\n",
    "        + torch.arange(config.data.num_cat - 1, 0, -1, device=config.device).float()\n",
    "    )\n",
    "else:\n",
    "    s = torch.ones(config.data.num_cat - 1, device=config.device)\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = get_mnist_dataset(config)\n",
    "valid_dataloader = None\n",
    "# time_dependent_weights = torch.load(config.loading.time_dep_weights_path)\n",
    "\"\"\"\n",
    "if not os.path.exists(config.saving.time_dep_weights_path):\n",
    "    os.makedirs(config.saving.time_dep_weights_path)\n",
    "str_speed = \".speed_balance\" if config.speed_balanced  else \"\"\n",
    "str_random_order = \".random_order\" if config.random_order else \"\"\n",
    "filename = (f\"time_depend_weights_steps{config.n_time_steps}.cat{config.data.num_cat}{str_speed}{str_random_order}\")\n",
    "filepath = os.path.join(config.saving.time_dep_weights_path, filename + \".pth\")\n",
    "torch.save(time_dependent_weights, filepath)\n",
    "\"\"\"\n",
    "time_dependent_weights = torch.load(config.loading.time_dep_weights_path)\n",
    "plt.plot(np.arange(1, config.n_time_steps + 1), time_dependent_weights.cpu())\n",
    "plt.title(\"Time Dependent Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTScoreNet(ch=config.model.ch, ch_mult=config.model.ch_mult, attn=config.model.attn, num_res_blocks=config.model.num_res_blocks, dropout=0.1, time_dependent_weights=time_dependent_weights)\n",
    "print(\"Number of parameters: \", sum([p.numel() for p in model.parameters()]))\n",
    "optimizer = Adam(model.parameters(), lr=config.optimizer.lr, weight_decay=config.optimizer.weight_decay)\n",
    "n_iter = 0\n",
    "state = {\"model\": model, \"optimizer\": optimizer, \"n_iter\": 0}\n",
    "\n",
    "if train_resume:\n",
    "    checkpoint_path = config.saving.checkpoint_path\n",
    "    model_name = 'model_7999_old.pt'\n",
    "    checkpoint_path = os.path.join(path, date, model_name)\n",
    "    state = bookkeeping.load_state(state, checkpoint_path)\n",
    "    config.training.n_iter = 9000\n",
    "    config.sampler.sampler_freq = 9000\n",
    "    config.saving.checkpoint_freq = 1000\n",
    "print(config.training.n_iter, state['n_iter'])\n",
    "sampler = Euler_Maruyama_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "# epochen umrechnen in n_iter => epoch = 20 and dataset 10000 => n_iter = 20 * 10000\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "    exit_flag = False\n",
    "\n",
    "    #stime = time.time()\n",
    "\n",
    "    for x_train in tqdm(train_dataloader, desc='Training'):\n",
    "        #print(\"before:\", x_train.shape)\n",
    "        #print(\"squeeze\", x_train.squeeze().long().shape)\n",
    "        #x_train = binary_to_onehot(x_train.squeeze())\n",
    "        x_train = F.one_hot(x_train.squeeze().long(), num_classes=config.data.num_cat)\n",
    "        #print(\"after\", x_train.shape)\n",
    "\n",
    "        # Optional : there are several options for importance sampling here. it needs to match the loss function\n",
    "        random_t = torch.LongTensor(np.random.choice(np.arange(config.n_time_steps), size=x_train.shape[0],\n",
    "                                                    p=(torch.sqrt(time_dependent_weights) / torch.sqrt(\n",
    "                                                        time_dependent_weights).sum()).cpu().detach().numpy()))\n",
    "        # noise data\n",
    "        \"\"\"\n",
    "        if self.config.random_order:\n",
    "            order = np.random.permutation(np.arange(self.config.data.num_cat))\n",
    "            # perturbed_x, perturbed_x_grad = diffusion_fast_flatdirichlet(x[...,order], random_t, v_one, v_one_loggrad)\n",
    "            #perturbed_x, perturbed_x_grad = diffusion_factory(x[..., order], random_t, v_one, v_zero, v_one_loggrad, v_zero_loggrad, alpha, beta) used diffusion facoty\n",
    "            perturbed_x, perturbed_x_grad = self.diffuser_func(x=x_train[..., order], time_ind=random_t)\n",
    "\n",
    "            perturbed_x = perturbed_x[..., np.argsort(order)]\n",
    "            perturbed_x_grad = perturbed_x_grad[..., np.argsort(order)]\n",
    "        else:\n",
    "        \"\"\"\n",
    "        perturbed_x, perturbed_x_grad = diffuser_func(x=x_train.cpu(), time_ind=random_t) # used this: Flat dirichlet\n",
    "        # perturbed_x, perturbed_x_grad = diffusion_factory(x, random_t, v_one, v_zero, v_one_loggrad, v_zero_loggrad, alpha, beta)\n",
    "\n",
    "        perturbed_x = perturbed_x.to(config.device)\n",
    "        perturbed_x_grad = perturbed_x_grad.to(config.device)\n",
    "        random_timepoints = timepoints[random_t].to(config.device)\n",
    "\n",
    "        random_t = random_t.to(config.device)\n",
    "\n",
    "        # Ã¤nderung hier kein cat x, s\n",
    "        # predict noise?\n",
    "        score = state['model'](perturbed_x, random_timepoints)\n",
    "\n",
    "        # the loss weighting function may change, there are a few options that we will experiment o\n",
    "        \"\"\"\n",
    "        if self.config.random_order:\n",
    "            order = np.random.permutation(np.arange(self.config.data.num_cat))\n",
    "            perturbed_v = self.sb._inverse(perturbed_x[..., order], prevent_nan=True).detach()\n",
    "            loss = torch.mean(torch.mean(\n",
    "                1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_train.ndim - 1)] * self.s[\n",
    "                    (None,) * (x_train.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (\n",
    "                            gx_to_gv(score[..., order], perturbed_x[..., order], create_graph=True) - gx_to_gv(\n",
    "                        perturbed_x_grad[..., order], perturbed_x[..., order])) ** 2, dim=(1)))\n",
    "        else:\n",
    "        \"\"\"\n",
    "        perturbed_v = sb._inverse(perturbed_x, prevent_nan=True).detach()\n",
    "        loss = torch.mean(torch.mean(\n",
    "            1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_train.ndim - 1)] * s[\n",
    "                (None,) * (x_train.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (\n",
    "                        gx_to_gv(score, perturbed_x, create_graph=True) - gx_to_gv(perturbed_x_grad, perturbed_x)) ** 2, dim=(1)))\n",
    "        #loss = loss_fn(x_train, perturbed_x, perturbed_x_grad, perturbed_v, score, self.s, important_sampling_weights=time_dependent_weights)\n",
    "        \n",
    "        state['optimizer'].zero_grad()\n",
    "        loss.backward()\n",
    "        state['optimizer'].step()\n",
    "        avg_loss += loss.item() * x_train.shape[0]\n",
    "        num_items += x_train.shape[0]\n",
    "        #print(\"Loss:\", loss.item())\n",
    "\n",
    "        if (valid_dataloader is not None) and ((state['n_iter'] + 1) % config.training.validation_freq == 0): # 5 => 5 * n_iter\n",
    "            state['model'].eval()\n",
    "            valid_avg_loss = 0.0\n",
    "            valid_num_items = 0\n",
    "            \n",
    "            #with torch.no_grad():\n",
    "            print(\"Validation:\")\n",
    "            for x_valid in tqdm(valid_dataloader, desc='Validation'):\n",
    "                \n",
    "                x_valid = F.one_hot(x_valid.squeeze().long(), num_classes=config.data.num_cat)\n",
    "                random_t = torch.LongTensor(np.random.choice(np.arange(config.n_time_steps),size=x_valid.shape[0],p=(torch.sqrt(time_dependent_weights) / torch.sqrt(time_dependent_weights).sum()).cpu().detach().numpy())).to(device)\n",
    "                \n",
    "                perturbed_x, perturbed_x_grad = diffuser_func(x_valid, random_t)\n",
    "                # perturbed_x, perturbed_x_grad = diffusion_fast_flatdirichlet(x, random_t, v_one, v_one_loggrad)\n",
    "                \n",
    "                perturbed_x = perturbed_x.to(device)\n",
    "                perturbed_x_grad = perturbed_x_grad.to(device)\n",
    "                random_t = random_t.to(device)\n",
    "                random_timepoints = timepoints[random_t]\n",
    "\n",
    "                score = state['model'](perturbed_x, random_timepoints)            \n",
    "                perturbed_v = sb._inverse(perturbed_x, prevent_nan=True).detach()\n",
    "                val_loss = torch.mean(torch.mean(1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_valid.ndim - 1)] * s[(None,) * (x_valid.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (gx_to_gv(score, perturbed_x, create_graph=True) - gx_to_gv(perturbed_x_grad, perturbed_x)) ** 2, dim=(1)))\n",
    "                #val_loss = loss_fn(x_valid, perturbed_x, perturbed_x_grad, perturbed_v, score, self.s, important_sampling_weights=time_dependent_weights)\n",
    "                valid_avg_loss += val_loss.item() * x_valid.shape[0]\n",
    "                valid_num_items += x_valid.shape[0]\n",
    "\n",
    "            print(\"Average Validation Loss\", valid_avg_loss / valid_num_items)\n",
    "            state['model'].train()\n",
    "\n",
    "        if (state['n_iter'] + 1) % config.sampler.sampler_freq == 0 or state['n_iter'] ==  config.training.n_iter- 1: \n",
    "            state['model'].eval()\n",
    "            print(\"Sampling:\")\n",
    "            samples = sampler(state['model'], config.data.shape, batch_size=config.sampler.n_samples, max_time=4, min_time=0.01, num_steps=100, eps=1e-5, random_order=config.random_order, speed_balanced=config.speed_balanced, device=config.device)\n",
    "            # (28, 28, 2)\n",
    "            ## Sample visualization.\n",
    "            #print(\"samples before\", samples, samples.shape)\n",
    "            samples = samples.clamp(0.0, config.data.num_cat)\n",
    "            #print(\"samples after\", samples, samples.shape)\n",
    "            sample_grid = make_grid(samples[:,None, :,:,0].detach().cpu(), nrow=int(np.sqrt(config.sampler.n_samples)))\n",
    "\n",
    "            # plot \n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(sample_grid.permute(1, 2, 0).cpu())\n",
    "            #plt.show()\n",
    "            # save\n",
    "            saving_plot_path = os.path.join(config.saving.sample_plot_path, f\"samples_epoch_{state['n_iter']}.png\")\n",
    "            plt.savefig(saving_plot_path)\n",
    "            plt.close()\n",
    "            \n",
    "            state['model'].train()\n",
    "\n",
    "        if (state['n_iter'] + 1) % config.saving.checkpoint_freq == 0 or state['n_iter']== config.training.n_iter - 1:\n",
    "            bookkeeping.save_state(state, config.saving.save_location)\n",
    "\n",
    "        if config.training.n_iter == state['n_iter'] - 1:\n",
    "            exit_flag = True\n",
    "            break\n",
    "\n",
    "        state['n_iter'] += 1\n",
    "\n",
    "    print(\"Average Loss after 1 epoch:\", avg_loss / num_items)\n",
    "    if exit_flag:\n",
    "        break\n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
