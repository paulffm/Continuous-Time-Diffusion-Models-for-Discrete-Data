{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lib.models.models import UniformRate, UniformVariantRate\n",
    "\n",
    "from lib.utils import utils\n",
    "import torch.nn.functional as F\n",
    "from lib.networks.unet import UNet\n",
    "from lib.networks.hollow_networks import BidirectionalTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "import lib.losses.losses as losses\n",
    "import lib.losses.losses_utils as losses_utils\n",
    "import lib.training.training as training\n",
    "import lib.training.training_utils as training_utils\n",
    "import lib.optimizers.optimizers as optimizers\n",
    "import lib.optimizers.optimizers_utils as optimizers_utils\n",
    "import lib.loggers.loggers as loggers\n",
    "import lib.loggers.logger_utils as logger_utils\n",
    "import lib.sampling.sampling as sampling\n",
    "import lib.sampling.sampling_utils as sampling_utils\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import lib.sampling.sampling_utils as sampling_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0', dtype=torch.int32) torch.Size([640, 32])\n",
      "xrep tensor([[1, 0, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 1,  ..., 0, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 1, 0],\n",
      "        [1, 1, 0,  ..., 1, 1, 1]], device='cuda:0') torch.Size([640, 32])\n",
      "candidate tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0') torch.Size([20])\n",
      "candidate 2 tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0') torch.Size([640, 1])\n",
      "xall tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 1, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 1,  ..., 0, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 1, 1],\n",
      "        [1, 1, 0,  ..., 1, 1, 1]], device='cuda:0') torch.Size([640, 32])\n",
      "t tensor([0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123,\n",
      "        0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515,\n",
      "        0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566,\n",
      "        0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826, 0.3921,\n",
      "        0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436, 0.4826,\n",
      "        0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085, 0.3436,\n",
      "        0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142, 0.5085,\n",
      "        0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891, 0.2142,\n",
      "        0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625, 0.7891,\n",
      "        0.2142, 0.5085, 0.3436, 0.4826, 0.3921, 0.0566, 0.1515, 0.7123, 0.6625,\n",
      "        0.7891], device='cuda:0') torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "D = 32\n",
    "ddim = D\n",
    "B = 10\n",
    "bsize = B\n",
    "S = 2\n",
    "device = 'cuda'\n",
    "vocab_size = S\n",
    "#mask = jnp.eye(ddim, dtype=jnp.int32).repeat(bsize * vocab_size, axis=0)\n",
    "#print(mask, mask.shape)\n",
    "mask = torch.eye(D, device=device, dtype=torch.int32).repeat_interleave(B * S, 0)\n",
    "print(\"mask\", mask, mask.shape)\n",
    "min_time = 0.01\n",
    "ts = torch.rand((B,), device=device) * (1.0 - min_time) + min_time\n",
    "xt = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "\n",
    "xrep = torch.tile(xt, (D * S, 1))\n",
    "print(\"xrep\", xrep, xrep.shape)\n",
    "candidate = torch.arange(S, device=device).repeat_interleave(B, 0)\n",
    "print(\"candidate\", candidate, candidate.shape)\n",
    "candidate = torch.tile(candidate.unsqueeze(1), ((D, 1)))\n",
    "print(\"candidate 2\", candidate, candidate.shape)\n",
    "xall = mask * candidate + (1 - mask) * xrep\n",
    "print(\"xall\", xall, xall.shape)\n",
    "t = torch.tile(ts, (D * S,))\n",
    "print(\"t\", t, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask [[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]] (640, 32)\n",
      "xrep [[0 1 1 ... 1 1 1]\n",
      " [0 1 1 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 1 1]\n",
      " [1 0 1 ... 0 1 0]] (640, 32)\n",
      "candidate [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] (20,)\n",
      "candidate 2 [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]] (640, 1)\n",
      "xall [[0 1 1 ... 1 1 1]\n",
      " [0 1 1 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 1 ... 0 0 1]\n",
      " [1 1 1 ... 0 1 1]\n",
      " [1 0 1 ... 0 1 1]] (640, 32)\n",
      "t [0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564 0.35490513 0.60419905\n",
      " 0.4275843  0.23061597 0.32985854 0.43953657 0.25099766 0.27730572\n",
      " 0.7678207  0.71474564 0.35490513 0.60419905 0.4275843  0.23061597\n",
      " 0.32985854 0.43953657 0.25099766 0.27730572 0.7678207  0.71474564\n",
      " 0.35490513 0.60419905 0.4275843  0.23061597 0.32985854 0.43953657\n",
      " 0.25099766 0.27730572 0.7678207  0.71474564] (640,)\n"
     ]
    }
   ],
   "source": [
    "D = 32\n",
    "ddim = D\n",
    "B = 10\n",
    "bsize = B\n",
    "S = 2\n",
    "device = 'cuda'\n",
    "vocab_size = S\n",
    "rng = jax.random.PRNGKey(0)\n",
    "t = jax.random.uniform(rng, (bsize,))\n",
    "xt = jax.random.randint(rng, (B, D), minval=0,\n",
    "                        maxval=S, dtype=jnp.int32)\n",
    "mask = jnp.eye(ddim, dtype=jnp.int32).repeat(bsize * vocab_size, axis=0)\n",
    "print(\"mask\", mask, mask.shape)\n",
    "xrep = jnp.tile(xt, (ddim * vocab_size, 1))\n",
    "print(\"xrep\", xrep, xrep.shape)\n",
    "candidate = jnp.arange(vocab_size).repeat(bsize, axis=0)\n",
    "print(\"candidate\", candidate, candidate.shape)\n",
    "candidate = jnp.tile(jnp.expand_dims(candidate, axis=1), ((ddim, 1)))\n",
    "print(\"candidate 2\", candidate, candidate.shape)\n",
    "xall = mask * candidate + (1 - mask) * xrep\n",
    "print(\"xall\", xall, xall.shape)\n",
    "t = jnp.tile(t, (ddim * vocab_size,))\n",
    "print(\"t\", t, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import utils\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "S = 2\n",
    "N = 5\n",
    "D = 32\n",
    "device = 'cpu'\n",
    "xt = torch.randint(low=0, high=S, size=(N, D), device=device)\n",
    "choices = jnp.expand_dims(jnp.arange(S, dtype=jnp.int32),\n",
    "                            axis=list(range(xt.ndim)))\n",
    "print(choices, choices.shape)\n",
    "choices = utils.expand_dims(\n",
    "    torch.arange(S, dtype=torch.int32), axis=list(range(xt.ndim))\n",
    ")\n",
    "print(choices, choices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = '/Users/paulheller/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/'\n",
    "save_location = os.path.join(script_dir, 'SavedModels/Synthetic/')\n",
    "save_location_png = os.path.join(save_location, 'PNGs/')\n",
    "dataset_location = os.path.join(script_dir, 'lib/datasets/Synthetic/data_2spirals.npy')\n",
    "\n",
    "\n",
    "cfg = get_config()\n",
    "\n",
    "device = cfg.device\n",
    "dataset = dataset_utils.get_dataset(cfg, device, dataset_location)\n",
    "dataloader = DataLoader(dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=cfg.data.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm, inv_bm = dataset_utils.get_binmap(cfg.concat_dim, cfg.data.binmode)\n",
    "for samples in dataloader:\n",
    "    samples = samples.numpy()\n",
    "    samples = dataset_utils.bin2float(\n",
    "        samples.astype(np.int32), inv_bm, cfg.concat_dim, cfg.data.int_scale\n",
    "    )\n",
    "\n",
    "    saving_plot_path = os.path.join(\n",
    "        save_location_png,\n",
    "        f\"{cfg.loss.name}test_{cfg.sampler.name}{cfg.sampler.num_steps}.pdf\",\n",
    "    )\n",
    "    dataset_utils.plot_samples(\n",
    "        samples, saving_plot_path, im_size=4.1, im_fmt=\"pdf\"\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "x_mean = 0 # 127.5\n",
    "x_std = 0 # 73.7\n",
    "\n",
    "for minibatch in dataloader:\n",
    "    \n",
    "    B = minibatch.shape[0]\n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    device = cfg.device\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.0\n",
    "    ts = torch.ones((B, ), device=device) * 0\n",
    "    #print(ts)\n",
    "    #ts = torch.ones((B, )) \n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(torch.arange(B), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    xt = torch.distributions.categorical.Categorical(logits=log_qt0).sample()\n",
    "    xt = xt.numpy()\n",
    "    #print(type(xt))\n",
    "    samples = dataset_utils.bin2float(\n",
    "        xt.astype(np.int32), inv_bm, cfg.concat_dim, cfg.data.int_scale\n",
    "    )\n",
    "\n",
    "    saving_plot_path = os.path.join(\n",
    "        save_location_png,\n",
    "        f\"{cfg.loss.name}noisy_{cfg.sampler.name}{cfg.sampler.num_steps}.png\",\n",
    "    )\n",
    "    dataset_utils.plot_samples(\n",
    "        samples, saving_plot_path, im_size=cfg.data.plot_size, im_fmt=\"png\"\n",
    "    )\n",
    "\n",
    "    break\n",
    "xt = torch.randint(low=0, high=2, size=(1000, 32), device=device)\n",
    "xt = xt.numpy()\n",
    "    #print(type(xt))\n",
    "samples = dataset_utils.bin2float(\n",
    "    xt.astype(np.int32), inv_bm, cfg.concat_dim, cfg.data.int_scale\n",
    ")\n",
    "\n",
    "saving_plot_path = os.path.join(\n",
    "    save_location_png,\n",
    "    f\"{cfg.loss.name}realnoisy_{cfg.sampler.name}{cfg.sampler.num_steps}.png\",\n",
    ")\n",
    "dataset_utils.plot_samples(\n",
    "    samples, saving_plot_path, im_size=cfg.data.plot_size, im_fmt=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cpu'  # Beispielgert\n",
    "num_steps = 100\n",
    "max_time = 1\n",
    "min_time = 0.01\n",
    "time_dilation = 2\n",
    "\n",
    "# Szenario 1: time_dilation_start_time ist None\n",
    "time_steps = torch.linspace(\n",
    "    max_time, min_time, num_steps * time_dilation + 1, device=device\n",
    ")\n",
    "step_sizes = time_steps[:-1] - time_steps[1:]\n",
    "time_steps = time_steps[:-1]\n",
    "\n",
    "print(\"Szenario 1 - Time Steps:\", time_steps, time_steps.shape)\n",
    "print(\"Szenario 1 - Step Sizes:\", step_sizes, step_sizes.shape)\n",
    "\n",
    "# Szenario 2: time_dilation_start_time ist 0.1\n",
    "time_dilation_start_time = 0.1\n",
    "num_steps_first = round(100 * (max_time - time_dilation_start_time) / max_time) + 1\n",
    "num_steps_second = round(100 * (time_dilation_start_time - min_time) / max_time) * time_dilation + 1\n",
    "\n",
    "time_steps_first = torch.linspace(max_time, time_dilation_start_time, num_steps_first, device=device)[:-1]\n",
    "time_steps_second = torch.linspace(time_dilation_start_time, min_time, num_steps_second, device=device)\n",
    "time_steps = torch.cat([time_steps_first, time_steps_second])\n",
    "\n",
    "step_sizes = time_steps[:-1] - time_steps[1:]\n",
    "time_steps = time_steps[:-1]\n",
    "\n",
    "print(\"Szenario 2 - Time Steps:\", time_steps, time_steps.shape)\n",
    "print(\"Szenario 2 - Step Sizes:\", step_sizes, step_sizes.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
