{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lib.datasets.synthetic as synthetic\n",
    "import torchvision\n",
    "import lib.datasets.maze as maze\n",
    "import lib.datasets.mnist as mnist\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from pathlib import Path\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "import lib.losses.losses as losses\n",
    "import lib.losses.losses_utils as losses_utils\n",
    "import lib.training.training as training\n",
    "import lib.training.training_utils as training_utils\n",
    "import lib.optimizers.optimizers as optimizers\n",
    "import lib.optimizers.optimizers_utils as optimizers_utils\n",
    "import lib.loggers.loggers as loggers\n",
    "import lib.loggers.logger_utils as logger_utils\n",
    "from lib.models.models import UniformRate, UniformVariantRate, GaussianTargetRate\n",
    "import lib.utils.utils as utils\n",
    "import numpy as np\n",
    "from lib.datasets import dataset_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def show_images(images, n=8):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0).numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.savefig(\"Mazes2.png\")\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_samples(samples, im_size=0, axis=False, im_fmt=None):\n",
    "    \"\"\"Plot samples.\"\"\"\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], marker=\".\")\n",
    "    plt.axis(\"equal\")\n",
    "    if im_size > 0:\n",
    "        plt.xlim(-im_size, im_size)\n",
    "        plt.ylim(-im_size, im_size)\n",
    "    if not axis:\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# loss_res = np.load('loss_CatRM104999_hollow10MRRes.npy')\n",
    "def apply_ema(signal, alpha):\n",
    "    \"\"\"\n",
    "    Anwendung des Exponential Moving Average auf ein Signal.\n",
    "\n",
    "    :param signal: Das Eingangssignal (Liste oder NumPy-Array).\n",
    "    :param alpha: Der Glättungsfaktor. Niedrigere Werte glätten mehr, typisch zwischen 0 und 1.\n",
    "    :return: Das geglättete Signal.\n",
    "    \"\"\"\n",
    "    ema = [signal[0]]  # Startwert für EMA\n",
    "    for i in range(1, len(signal)):\n",
    "        ema.append(alpha * signal[i] + (1 - alpha) * ema[i - 1])\n",
    "    return np.array(ema)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = loss_res[1000:105000]\n",
    "sm = apply_ema(res, 0.0001)\n",
    "plt.plot(sm)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_squared(n_samples, samples, save_location_png):\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(int(np.sqrt(n_samples)), int(np.sqrt(n_samples)), 1 + i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(samples[i, ...], (1, 2, 0)), cmap=\"gray\")\n",
    "\n",
    "    saving_plot_path = os.path.join(\n",
    "        save_location_png,\n",
    "        \"mazes.png\",\n",
    "    )\n",
    "    # plt.suptitle('Mazes', fontsize=16)\n",
    "    # plt.subplots_adjust(top=0.85)\n",
    "    # plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    # plt.savefig(saving_plot_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = utils.expand_dims(\n",
    "    torch.arange(10, device=\"cuda\", dtype=torch.int32),\n",
    "    axis=list(range(3)),\n",
    ")\n",
    "print(\"choic\", choices, choices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "device = \"cuda\"\n",
    "att_l2r_mask = ~torch.triu(\n",
    "    torch.ones((seq_len, seq_len), device=device, dtype=torch.bool),\n",
    "    diagonal=1,\n",
    ").unsqueeze(\n",
    "    0\n",
    ")  # 1, D, D\n",
    "\n",
    "att_r2l_mask = ~torch.tril(\n",
    "    torch.ones((seq_len, seq_len), device=device, dtype=torch.bool),\n",
    "    diagonal=-1,\n",
    ").unsqueeze(\n",
    "    0\n",
    ")  # 1, D, D\n",
    "\n",
    "\n",
    "att_t = torch.ones((1, seq_len, 1), device=device)  # 1, D, 1\n",
    "\n",
    "joint_mask = torch.cat([att_t, att_l2r_mask, att_r2l_mask], dim=-1).unsqueeze(\n",
    "    0\n",
    ")  # 1, 1, B, H*HD\n",
    "# print(\"joint_maks\", joint_mask.shape)\n",
    "joint_mask = (joint_mask > 0) * 1\n",
    "print(joint_mask, joint_mask.shape)\n",
    "\n",
    "\n",
    "seq_len = 10\n",
    "device = \"cuda\"\n",
    "att_l2r_mask = ~torch.triu(\n",
    "    torch.ones((seq_len, seq_len), device=device, dtype=torch.bool),\n",
    "    diagonal=1,\n",
    ")  # 1, D, D\n",
    "\n",
    "att_r2l_mask = ~torch.tril(\n",
    "    torch.ones((seq_len, seq_len), device=device, dtype=torch.bool),\n",
    "    diagonal=-1,\n",
    ")  # 1, D, D\n",
    "\n",
    "\n",
    "att_t = torch.ones((seq_len, 1), device=device)  # 1, D, 1\n",
    "\n",
    "joint_mask = torch.cat([att_t, att_l2r_mask, att_r2l_mask], dim=-1)  # 1, 1, B, H*HD\n",
    "# print(\"joint_maks\", joint_mask.shape)\n",
    "joint_mask = joint_mask > 0\n",
    "joint_mask = joint_mask.unsqueeze(0) * 1\n",
    "print(joint_mask, joint_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "device = \"cuda\"\n",
    "seq_len = 10\n",
    "idx = jnp.arange(10, dtype=jnp.int32)\n",
    "att_l2r_mask_jnp = nn.attention.make_attention_mask(idx, idx, jnp.greater_equal)\n",
    "\n",
    "att_l2r_mask = ~torch.triu(\n",
    "    torch.ones((seq_len, seq_len), device=device, dtype=torch.bool), diagonal=1\n",
    ")\n",
    "# print(\"torch\", att_l2r_mask * 1)\n",
    "# att_l2r_mask = ~att_l2r_mask\n",
    "\n",
    "\n",
    "idx = jnp.arange(seq_len, dtype=jnp.int32)\n",
    "att_l2r_mask = nn.attention.make_attention_mask(idx, idx, jnp.greater_equal)\n",
    "att_r2l_mask = nn.attention.make_attention_mask(idx, idx, jnp.less_equal)\n",
    "att_t = jnp.ones((1, seq_len, 1))\n",
    "joint_mask = jnp.concatenate([att_t, att_l2r_mask, att_r2l_mask], axis=-1)\n",
    "joint_mask = jnp.expand_dims(joint_mask, axis=0)\n",
    "print(joint_mask, joint_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = jnp.expand_dims(jnp.arange(10, dtype=jnp.int32), 0)  # 1, D(+1)\n",
    "\n",
    "mask_l2r_jnp = nn.attention.make_attention_mask(pos_idx, pos_idx, jnp.greater_equal)\n",
    "print(mask_l2r_jnp, mask_l2r_jnp.shape)\n",
    "mask_l2r_jnp = mask_l2r_jnp.at[:, :, :1, :1].set(1.0)\n",
    "print(mask_l2r_jnp, mask_l2r_jnp.shape)\n",
    "mask_l2r_torch = torch.triu(\n",
    "    torch.ones((10, 10), device=device, dtype=torch.bool),\n",
    "    diagonal=1,\n",
    ")\n",
    "mask_l2r_torch = torch.where(\n",
    "    mask_l2r_torch, torch.tensor(float(\"-inf\")), torch.tensor(0.0)\n",
    ")\n",
    "print(mask_l2r_torch, mask_l2r_torch.shape, mask_l2r_torch[5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_l2r_jnp = torch.triu(\n",
    "    torch.ones((10, 10), dtype=torch.bool),\n",
    "    diagonal=1,  # concat_dim = D; if conditioner:\n",
    ")\n",
    "print(mask_l2r_jnp)\n",
    "mask_l2r_jnp = torch.where(mask_l2r_jnp, torch.tensor(float(\"-inf\")), torch.tensor(0.0))\n",
    "att_r2l_mask = torch.tril(\n",
    "    torch.ones((10, 10), dtype=torch.bool),\n",
    "    diagonal=-1,\n",
    ")\n",
    "print(att_r2l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_l2r_jnp = torch.full((10, 10), float(\"-inf\"))\n",
    "mask_l2r_jnp[:5, :5] = 0\n",
    "print(mask_l2r_jnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM23999_hollow14MRTAtt64.npy\")\n",
    "# res = np.load('loss_CatRM11999_hollow20MRes.npy')\n",
    "att_10M = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM11999_hollow14MRTAtt.npy\")  # 93 000\n",
    "att_emb = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM5999_hollowMRTEmb.npy\")\n",
    "att_cos1 = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM5999_hollowMRT14Cos.npy\")\n",
    "att_cos10l_15mlp = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM5999.npy\")\n",
    "\n",
    "print(len(att_10M))\n",
    "att = att[10000:12000]\n",
    "print(\"att64\", np.mean(att))\n",
    "a = 1000\n",
    "b = 6000\n",
    "att_emb = att_emb[a:b]\n",
    "print(\"EMB\", np.mean(att_emb))\n",
    "# print(\"Cos high LR\", np.mean(att_cos_highlr))\n",
    "att_cos1 = att_cos1[a:b]\n",
    "print(\"Cos1\", np.mean(att_cos1))\n",
    "att_cos10l_15mlp = att_cos10l_15mlp[a:b]\n",
    "print(\"Cos10L\", np.mean(att_cos10l_15mlp))\n",
    "att_10M = att_10M[a:b]\n",
    "print(\"att128\", np.mean(att_10M))\n",
    "\n",
    "coff = 0.5\n",
    "\n",
    "att_sm = apply_ema(att, 1)\n",
    "att_cos1 = apply_ema(att_cos1, coff)\n",
    "att10M_sm = apply_ema(att_10M, coff)\n",
    "att_emb = apply_ema(att_emb, coff)\n",
    "att_cos10l_15mlp = apply_ema(att_cos10l_15mlp, coff)\n",
    "# plt.plot(att_sm, label='att 64B')\n",
    "# plt.plot(att, label='att 64B')\n",
    "plt.plot(att_emb, label=\"EMb\")\n",
    "plt.plot(att_cos1, label=\"Cos\")\n",
    "plt.plot(att_cos10l_15mlp, label=\"Cos 2e-4\")\n",
    "# plt.plot(att_10M, label='att 128B')\n",
    "# plt.plot(att_sm, label='att 64B')\n",
    "plt.plot(att10M_sm, label=\"att 128B\")\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##att_20MR = np.load('SavedModels/MAZE/PNGs/loss_CatRM5999.npy')\n",
    "att_14MRT = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM56999.npy\")\n",
    "# att_14MRT2 = np.load('SavedModels/MAZE/PNGs/loss_CatRM10999_hollowXtCos141e4.npy')\n",
    "att_14MRT3 = np.load(\"SavedModels/MAZE/PNGs/loss_CatRM74999_hollowXtCos14.npy\")\n",
    "\n",
    "a = 25000\n",
    "b = 56999\n",
    "coff = 0.001\n",
    "\n",
    "# att_20MR = att_20MR[a:b]\n",
    "att_14MRT = att_14MRT[a:b]\n",
    "# att_14MRT2 = att_14MRT2[a:b]\n",
    "att_14MRT3 = att_14MRT3[a:b]\n",
    "# att_20MR = apply_ema(att_20MR, coff)\n",
    "\n",
    "att_14MRT = apply_ema(att_14MRT, coff)\n",
    "# att_14MRT2 = apply_ema(att_14MRT2, coff)\n",
    "att_14MRT3 = apply_ema(att_14MRT3, coff)\n",
    "plt.plot(att_14MRT, label=\"RProb clip31.5e4\")\n",
    "# plt.plot(att_14MRT2, label='Cos right')\n",
    "plt.plot(att_14MRT3, label=\"Cos wron 2e4\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_tauUnet_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "model = model_utils.create_model(cfg, cfg.device)\n",
    "print(\"Number of Parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_res = np.load(\"loss_CatRM104999_hollow10MRRes.npy\")\n",
    "\n",
    "\n",
    "def apply_ema(signal, alpha):\n",
    "    \"\"\"\n",
    "    Anwendung des Exponential Moving Average auf ein Signal.\n",
    "\n",
    "    :param signal: Das Eingangssignal (Liste oder NumPy-Array).\n",
    "    :param alpha: Der Glättungsfaktor. Niedrigere Werte glätten mehr, typisch zwischen 0 und 1.\n",
    "    :return: Das geglättete Signal.\n",
    "    \"\"\"\n",
    "    ema = [signal[0]]  # Startwert für EMA\n",
    "    for i in range(1, len(signal)):\n",
    "        ema.append(alpha * signal[i] + (1 - alpha) * ema[i - 1])\n",
    "    return np.array(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_bert_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "model = model_utils.create_model(cfg, cfg.device)\n",
    "print(\"Number of Parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.networks.ddsm_networks import ProteinScoreNet\n",
    "from config.maze_config.config_hollow_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "cfg.device = \"cuda\"\n",
    "\n",
    "\n",
    "D = cfg.model.concat_dim = 1024\n",
    "S = cfg.data.S = 5\n",
    "B = cfg.data.batch_size\n",
    "net = ProteinScoreNet(cfg).to(cfg.device)\n",
    "ts = torch.rand((B,), device=cfg.device) * (1.0 - 0.01) + 0.01\n",
    "x = torch.randint(low=0, high=S, size=(B, D), device=\"cuda\")\n",
    "x_out = net(x, ts)\n",
    "print(x_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.mnist_config.config_bert_mnist import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, cfg.data.location)\n",
    "cfg.data.batch_size = 1\n",
    "mnist_dl = DataLoader(\n",
    "    mnist_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)\n",
    "cfg.model.time_base = 3\n",
    "cfg.model.time_exp = 100\n",
    "cfg.model.rate_sigma = 6.0\n",
    "cfg.model.Q_sigma = 512.0\n",
    "\n",
    "model = GaussianTargetRate(cfg, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0.1, 1, 10)\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    for ts in t:\n",
    "        ts = torch.ones((B,), device=device) * ts\n",
    "        qt0 = model.transition(\n",
    "            ts\n",
    "        )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "        # R_t = beta_t * R_b\n",
    "        rate = model.rate(\n",
    "            ts\n",
    "        )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "        qt0_rows_reg = qt0[\n",
    "            torch.arange(B, device=device).repeat_interleave(\n",
    "                D\n",
    "            ),  # repeats every element 0 to B-1 D-times\n",
    "            minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "            :,\n",
    "        ]  # (B*D, S)\n",
    "        b = utils.expand_dims(\n",
    "            torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "        )\n",
    "        qt0 = qt0[b, minibatch.long()]\n",
    "        print(qt0_rows_reg[0], qt0_rows_reg.shape)\n",
    "        # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "        x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.synthetic_config.config_ebm_synthetic import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "location = \"lib/datasets/Synthetic/data_2spirals.npy\"\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "synthetic_dataset = dataset_utils.get_dataset(cfg, device, location)\n",
    "cfg.data.batch_size = 1\n",
    "synthetic_dl = DataLoader(\n",
    "    synthetic_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 2.5  # 0.00009\n",
    "cfg.model.t_func = \"log_sqr\"\n",
    "# model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 2\n",
    "bm, inv_bm = synthetic.get_binmap(cfg.model.concat_dim, cfg.data.binmode)\n",
    "ls = np.linspace(0.001, 1, 10)\n",
    "for minibatch in synthetic_dl:\n",
    "    for i in ls:\n",
    "        # print(minibatch.device)\n",
    "        B, D = minibatch.shape\n",
    "        minibatch = minibatch.view(B, D).to(\"cpu\")\n",
    "        ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "        ts = torch.ones((B,), device=device) * i\n",
    "        print(\"Time points\", ts[:9])\n",
    "\n",
    "        qt0 = model.transition(\n",
    "            ts\n",
    "        )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "        # R_t = beta_t * R_b\n",
    "        rate = model.rate(\n",
    "            ts\n",
    "        )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "        qt0_rows_reg = qt0[\n",
    "            torch.arange(B, device=device).repeat_interleave(\n",
    "                D\n",
    "            ),  # repeats every element 0 to B-1 D-times\n",
    "            minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "            :,\n",
    "        ]  # (B*D, S)\n",
    "        b = utils.expand_dims(\n",
    "            torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "        )\n",
    "        qt0 = qt0[b, minibatch.long()]\n",
    "        print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "        # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "        x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "        x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "            B, D\n",
    "        )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "        rate_vals_square = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "        ]  # (B*D, S)\n",
    "\n",
    "        rate_vals_square[\n",
    "            torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "        ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "        rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "        rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "            B, D\n",
    "        )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "        square_dimcat = torch.distributions.categorical.Categorical(\n",
    "            rate_vals_square_dimsum\n",
    "        )\n",
    "\n",
    "        square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "        rate_new_val_probs = rate_vals_square[\n",
    "            torch.arange(B, device=device), square_dims, :\n",
    "        ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "        # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "        # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "        square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "            rate_new_val_probs\n",
    "        )\n",
    "\n",
    "        square_newval_samples = (\n",
    "            square_newvalcat.sample()\n",
    "        )  # (B, ) taking values in [0, S)\n",
    "\n",
    "        x_tilde = x_t.clone()\n",
    "        x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "        x_tilde = synthetic.bin2float(\n",
    "            x_tilde.detach().cpu().numpy().astype(np.int32),\n",
    "            inv_bm,\n",
    "            cfg.model.concat_dim,\n",
    "            cfg.data.int_scale,\n",
    "        )\n",
    "\n",
    "        # minibatch = synthetic.bin2float(minibatch.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "        # plot_samples(minibatch) # * 127.5\n",
    "        # plot_samples(x_tilde) # * 127.5\n",
    "        noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "\n",
    "        # noise_x = synthetic.bin2float(noise_x.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "        # plot_samples(noise_x) # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.1  # 0.00009\n",
    "cfg.model.t_func = \"log\"\n",
    "\n",
    "cfg.model.time_base = 3\n",
    "cfg.model.time_exp = 100\n",
    "cfg.model.rate_sigma = 6.0\n",
    "cfg.model.Q_sigma = 512.0\n",
    "# model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 256\n",
    "\n",
    "\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    print(minibatch.device)\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D).to(\"cpu\")\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * 1\n",
    "    print(\"Time points\", ts[:9])\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "    print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(rate_vals_square_dimsum)\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(rate_new_val_probs)\n",
    "\n",
    "    square_newval_samples = square_newvalcat.sample()  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu(), n=9)  # * 127.5\n",
    "    show_images(x_tilde.detach().cpu(), n=9)  # * 127.5\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W).detach().cpu(), n=9)  # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_hollow_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "maze_dataset = dataset_utils.get_dataset(cfg, device)\n",
    "cfg.data.batch_size = 10\n",
    "maze_dl = DataLoader(\n",
    "    maze_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 1.7  # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "cfg.model.time_base = 1\n",
    "cfg.model.time_exp = 250\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "t = torch.ones((B,), device=device) * 0.5\n",
    "qt0_x2y = model.transition(t)\n",
    "qt0_y2x = qt0_x2y.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((qt0_x2y == qt0_y2x).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        mask_reg = torch.ones((B, D, S), device=device)\n",
    "        mask_reg[\n",
    "            torch.arange(B, device=device).repeat_interleave(D),\n",
    "            torch.arange(D, device=device).repeat(B),\n",
    "            reg_x.long().flatten(),\n",
    "        ] = 0.0    \n",
    "rate_vals_reg = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D),\n",
    "            :,\n",
    "            reg_x.long().flatten(),\n",
    "        ].view(B, D, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(i))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m show_images(i\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu() \u001b[39m*\u001b[39m \u001b[39m127.5\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plot_squared(\u001b[39m25\u001b[39;49m, i\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqrt(n_samples)), \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqrt(n_samples)), \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m i)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mtranspose(samples[i, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)), cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m saving_plot_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     save_location_png,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmazes.png\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# plt.suptitle('Mazes', fontsize=16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# plt.subplots_adjust(top=0.85)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# plt.subplots_adjust(wspace=0.1, hspace=0.1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# plt.savefig(saving_plot_path)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in maze_dl:\n",
    "    print(type(i))\n",
    "    show_images(i.detach().cpu() * 127.5, n=10)\n",
    "    plot_squared(25, i.detach().cpu().numpy(), \"\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 1.7  # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "cfg.model.time_base = 1\n",
    "cfg.model.time_exp = 250\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "S = 3\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 10)\n",
    "tensor_list = []\n",
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    for i in ls:\n",
    "        minibatch = minibatch.view(B, D)\n",
    "        # ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "        ts = torch.ones((B,), device=device) * i\n",
    "        ts = torch.clamp(ts, max=0.99999)\n",
    "        # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "        print(\"Time points\", ts[:9])\n",
    "        qt0 = model.transition(\n",
    "            ts\n",
    "        )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "        # R_t = beta_t * R_b\n",
    "        rate = model.rate(\n",
    "            ts\n",
    "        )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "        qt0_rows_reg = qt0[\n",
    "            torch.arange(B, device=device).repeat_interleave(\n",
    "                D\n",
    "            ),  # repeats every element 0 to B-1 D-times\n",
    "            minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "            :,\n",
    "        ]  # (B*D, S)\n",
    "        b = utils.expand_dims(\n",
    "            torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "        )\n",
    "        qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "        print(qt0_rows_reg)  # , qt0)\n",
    "        # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "        x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "        x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "            B, D\n",
    "        )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "        rate_vals_square = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "        ]  # (B*D, S)\n",
    "\n",
    "        rate_vals_square[\n",
    "            torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "        ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "        rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "        rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "            B, D\n",
    "        )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "        square_dimcat = torch.distributions.categorical.Categorical(\n",
    "            rate_vals_square_dimsum\n",
    "        )\n",
    "\n",
    "        square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "        rate_new_val_probs = rate_vals_square[\n",
    "            torch.arange(B, device=device), square_dims, :\n",
    "        ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "        # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "        # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "        square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "            rate_new_val_probs\n",
    "        )\n",
    "\n",
    "        square_newval_samples = (\n",
    "            square_newvalcat.sample()\n",
    "        )  # (B, ) taking values in [0, S)\n",
    "\n",
    "        x_tilde = x_t.clone()\n",
    "        x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "        # x_tilde = x_tilde.view(B, C, H, W)\n",
    "        x_tilde = x_tilde.view(C, H, W)\n",
    "        tensor_list.append(x_tilde)\n",
    "        # show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=B) # * 127.5\n",
    "        # show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "        # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "        # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    tens = torch.stack(tensor_list, 0).to(device)\n",
    "    print(tens.shape)\n",
    "    show_images(tens.detach().cpu() * 127.5, n=len(ls))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu() * 127.5, n=8)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "\n",
    "D = 1\n",
    "S = 256\n",
    "x = torch.randint(low=0, high=S, size=(B, D))\n",
    "print(x.shape)\n",
    "x = F.one_hot(x.long(), S)\n",
    "out = x.permute(0, 2, 1).float()\n",
    "print(out.shape, type(out))\n",
    "# out = x\n",
    "lin = nn.Conv1d(S, 256 * 2, kernel_size=9, padding=4)\n",
    "print(lin(out).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.utils.utils as utils\n",
    "\n",
    "cfg.model.rate_const = 0.5\n",
    "cfg.model.t_func = \"log\"\n",
    "cfg.model.time_base = 5\n",
    "cfg.model.time_exp = 5\n",
    "model = UniformRate(cfg, \"cuda\")\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "device = \"cpu\"\n",
    "S = 3\n",
    "min_time = 0.01\n",
    "\n",
    "\n",
    "for minibatch in mnist_dataset:\n",
    "    if len(minibatch.shape) == 4:\n",
    "        B, C, H, W = minibatch.shape\n",
    "        minibatch = minibatch.view(B, C * H * W)\n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    B = minibatch.shape[0]\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - min_time) + min_time\n",
    "    ts = torch.ones((B,)) * 1\n",
    "    print(ts[:9])\n",
    "    #\n",
    "\n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    x_tilde = torch.distributions.categorical.Categorical(\n",
    "        logits=log_qt0\n",
    "    ).sample()  # bis hierhin <1 sek\n",
    "\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    print(torch.mean(x_tilde[1, :, :, :].float()))\n",
    "    # print(x_tilde[0,0, :, :].std())\n",
    "    show_images(minibatch.view(B, C, H, W) * 127.5, n=9)\n",
    "    show_images(x_tilde * 127.5, n=9)\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W) * 127.5, n=9)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int log(t**2 + 1)\n",
    "t = np.linspace(0.01, 1, 1000)\n",
    "f = 2 * t / (t**2 + 1)\n",
    "f_int = np.log(t**2 + 1)\n",
    "f_cos = np.sin(t) / np.sqrt(np.cos(t))\n",
    "a = 5\n",
    "b = 5\n",
    "f_log = a * np.log(b) * b**t\n",
    "plt.plot(f, label=\"log sqr\")\n",
    "plt.plot(f_int, label=\"int log sqr\")\n",
    "plt.plot(f_cos, label=\"cos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
