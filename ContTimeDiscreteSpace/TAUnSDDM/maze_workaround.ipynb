{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lib.datasets.synthetic as synthetic\n",
    "import torchvision\n",
    "import lib.datasets.maze as maze\n",
    "import lib.datasets.mnist as mnist\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from pathlib import Path\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "import lib.losses.losses as losses\n",
    "import lib.losses.losses_utils as losses_utils\n",
    "import lib.training.training as training\n",
    "import lib.training.training_utils as training_utils\n",
    "import lib.optimizers.optimizers as optimizers\n",
    "import lib.optimizers.optimizers_utils as optimizers_utils\n",
    "import lib.loggers.loggers as loggers\n",
    "import lib.loggers.logger_utils as logger_utils\n",
    "from lib.models.models import UniformRate, UniformVariantRate, GaussianTargetRate\n",
    "import lib.utils.utils as utils\n",
    "import numpy as np\n",
    "from lib.datasets import dataset_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def show_images(images, n=8, figsize=(10, 1), title='mazes.png'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0).numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.savefig(title, transparent=True)\n",
    "    plt.show()\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_samples(samples, im_size=0, axis=False, im_fmt=None):\n",
    "    \"\"\"Plot samples.\"\"\"\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], marker=\".\")\n",
    "    plt.axis(\"equal\")\n",
    "    if im_size > 0:\n",
    "        plt.xlim(-im_size, im_size)\n",
    "        plt.ylim(-im_size, im_size)\n",
    "    if not axis:\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# loss_res = np.load('loss_CatRM104999_hollow10MRRes.npy')\n",
    "def apply_ema(signal, alpha):\n",
    "    \"\"\"\n",
    "    Anwendung des Exponential Moving Average auf ein Signal.\n",
    "\n",
    "    :param signal: Das Eingangssignal (Liste oder NumPy-Array).\n",
    "    :param alpha: Der Glättungsfaktor. Niedrigere Werte glätten mehr, typisch zwischen 0 und 1.\n",
    "    :return: Das geglättete Signal.\n",
    "    \"\"\"\n",
    "    ema = [signal[0]]  # Startwert für EMA\n",
    "    for i in range(1, len(signal)):\n",
    "        ema.append(alpha * signal[i] + (1 - alpha) * ema[i - 1])\n",
    "    return np.array(ema)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = loss_res[1000:105000]\n",
    "sm = apply_ema(res, 0.0001)\n",
    "plt.plot(sm)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_squared(n_samples, samples, save_location_png):\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(int(np.sqrt(n_samples)), int(np.sqrt(n_samples)), 1 + i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(samples[i, ...], (1, 2, 0)), cmap=\"gray\")\n",
    "\n",
    "    saving_plot_path = os.path.join(\n",
    "        save_location_png,\n",
    "        \"mazes.png\",\n",
    "    )\n",
    "    # plt.suptitle('Mazes', fontsize=16)\n",
    "    # plt.subplots_adjust(top=0.85)\n",
    "    # plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    # plt.savefig(saving_plot_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.sudoku_config.config_sudoku_unet import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "cfg.data.name = 'SudokuDataset'\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, _)\n",
    "cfg.data.batch_size = 10\n",
    "mnist_dl = DataLoader(\n",
    "    mnist_dataset, cfg.data.batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)\n",
    "#cfg.model.time_base = 3\n",
    "#cfg.model.time_exp = 100\n",
    "#cfg.model.rate_sigma = 6.0\n",
    "#cfg.model.Q_sigma = 512.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.4 # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "S = 2\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 8)\n",
    "tensor_list = []\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) *1# 0.001\n",
    "    ts = torch.clamp(ts, max=0.99999)\n",
    "    # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "    print(\"Time points\", ts[:9])\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "    print(\"Prob\", qt0_rows_reg)  # , qt0)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    # ensures that positive\n",
    "\n",
    "    reg_x = x_tilde\n",
    "    outer_rate_sig = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D * S),\n",
    "        torch.arange(S, device=device).repeat(B * D),\n",
    "        x_tilde.long().flatten().repeat_interleave(S),\n",
    "    ].view(B, D, S)\n",
    "    rate_vals_reg = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D),\n",
    "            :,\n",
    "            reg_x.long().flatten(),\n",
    "        ].view(B, D, S)\n",
    "    print(\"Equal\", (outer_rate_sig == rate_vals_reg).all())\n",
    "    #tensor_list.append(x_tilde)\n",
    "    #show_images(minibatch.view(B, C, H, W).detach().cpu(), n=B) # * 127.5\n",
    "    #show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "#tens = torch.stack(tensor_list, 0).to(device)\n",
    "#tens[0] = minibatch.view(C, H, W)\n",
    "#print(tens.shape)\n",
    "#show_images(tens.detach().cpu(), n=len(ls))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.bin_mnist_config.config_tauUnet_binmnist import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "cfg.data.name = 'BinMNIST'\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, cfg.data.location)\n",
    "cfg.data.batch_size = 10\n",
    "mnist_dl = DataLoader(\n",
    "    mnist_dataset, cfg.data.batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)\n",
    "#cfg.model.time_base = 3\n",
    "#cfg.model.time_exp = 100\n",
    "#cfg.model.rate_sigma = 6.0\n",
    "#cfg.model.Q_sigma = 512.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 1.85 # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "S = 2\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 8)\n",
    "tensor_list = []\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * 1\n",
    "    ts = torch.clamp(ts, max=0.99999)\n",
    "    # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "    print(\"Time points\", ts[:9])\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "    print(\"Prob\", qt0_rows_reg)  # , qt0)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    # ensures that positive\n",
    "\n",
    "    reg_x = x_tilde\n",
    "    outer_rate_sig = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D * S),\n",
    "        torch.arange(S, device=device).repeat(B * D),\n",
    "        x_tilde.long().flatten().repeat_interleave(S),\n",
    "    ].view(B, D, S)\n",
    "    rate_vals_reg = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D),\n",
    "            :,\n",
    "            reg_x.long().flatten(),\n",
    "        ].view(B, D, S)\n",
    "    print(\"Equal\", (outer_rate_sig == rate_vals_reg).all())\n",
    "    #tensor_list.append(x_tilde)\n",
    "    #show_images(minibatch.view(B, C, H, W).detach().cpu(), n=B) # * 127.5\n",
    "    #show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "#tens = torch.stack(tensor_list, 0).to(device)\n",
    "#tens[0] = minibatch.view(C, H, W)\n",
    "#print(tens.shape)\n",
    "#show_images(tens.detach().cpu(), n=len(ls))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "patch_size = 4\n",
    "image_size_h = image_size_w = 28\n",
    "num_patches = (image_size_h // patch_size) * (image_size_w // patch_size)\n",
    "channels = 1\n",
    "patch_dim = channels * patch_size ** 2\n",
    "dim = patch_dim\n",
    "seq = nn.Sequential(\n",
    "    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "   #nn.LayerNorm(patch_dim),\n",
    "    nn.Linear(patch_dim, dim),\n",
    "    #nn.LayerNorm(dim)\n",
    ")\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "print(x)\n",
    "print(seq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 1  # Anzahl der Matrizen\n",
    "S = 3 # Anzahl der Zustände\n",
    "\n",
    "# Erzeuge eine NxSxS-Matrix mit Nullen\n",
    "state_change_matrix = torch.zeros(S, S)\n",
    "\n",
    "# Fülle die Hauptdiagonale mit Nullen\n",
    "state_change_matrix[torch.arange(S), torch.arange(S)] = 0\n",
    "\n",
    "# Fülle die Werte über der Hauptdiagonalen (nach rechts)\n",
    "for i in range(1, S):\n",
    "    state_change_matrix[torch.arange(S - i), torch.arange(i, S)] = i\n",
    "\n",
    "# Fülle die Werte unter der Hauptdiagonalen (nach links)\n",
    "for i in range(1, S):\n",
    "    state_change_matrix[torch.arange(i, S), torch.arange(S - i)] = -i\n",
    "print(state_change_matrix)\n",
    "B = 5\n",
    "# print(torch.tile(state_change_matrix, (B, 1,1)).shape)\n",
    "torch.save(state_change_matrix, 'state_change_matrix_maze.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.012 # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "S = 256\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 8)\n",
    "tensor_list = []\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    for i in ls:\n",
    "        minibatch = minibatch.view(B, D)\n",
    "        # ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "        ts = torch.ones((B,), device=device) * i\n",
    "        ts = torch.clamp(ts, max=0.99999)\n",
    "        # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "        print(\"Time points\", ts[:9])\n",
    "        qt0 = model.transition(\n",
    "            ts\n",
    "        )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "        # R_t = beta_t * R_b\n",
    "        rate = model.rate(\n",
    "            ts\n",
    "        )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "        qt0_rows_reg = qt0[\n",
    "            torch.arange(B, device=device).repeat_interleave(\n",
    "                D\n",
    "            ),  # repeats every element 0 to B-1 D-times\n",
    "            minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "            :,\n",
    "        ]  # (B*D, S)\n",
    "        b = utils.expand_dims(\n",
    "            torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "        )\n",
    "        qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "        print(qt0_rows_reg)  # , qt0)\n",
    "        # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "        x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "        x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "            B, D\n",
    "        )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "        rate_vals_square = rate[\n",
    "            torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "        ]  # (B*D, S)\n",
    "\n",
    "        rate_vals_square[\n",
    "            torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "        ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "        rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "        rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "            B, D\n",
    "        )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "        square_dimcat = torch.distributions.categorical.Categorical(\n",
    "            rate_vals_square_dimsum\n",
    "        )\n",
    "\n",
    "        square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "        rate_new_val_probs = rate_vals_square[\n",
    "            torch.arange(B, device=device), square_dims, :\n",
    "        ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "        # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "        # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "        square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "            rate_new_val_probs\n",
    "        )\n",
    "\n",
    "        square_newval_samples = (\n",
    "            square_newvalcat.sample()\n",
    "        )  # (B, ) taking values in [0, S)\n",
    "\n",
    "        x_tilde = x_t.clone()\n",
    "        x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "        # x_tilde = x_tilde.view(B, C, H, W)\n",
    "        x_tilde = x_tilde.view(C, H, W)\n",
    "        tensor_list.append(x_tilde)\n",
    "        # show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=B) # * 127.5\n",
    "        # show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "        # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "        # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    tens = torch.stack(tensor_list, 0).to(device)\n",
    "    tens[0] = minibatch.view(C, H, W)\n",
    "    print(tens.shape)\n",
    "    show_images(tens.detach().cpu(), n=len(ls))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images2(images, n=8, figsize=(3, 3), title='mazes.png'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(images.permute(1, 2, 0).numpy().astype(\"uint64\"), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(title)\n",
    "    plt.close()\n",
    "show_images2(tens[5].view(C, H, W).cpu(), n=1, title='middle_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images2(images, n=8, figsize=(3, 3), title='mazes.png'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(images.permute(1, 2, 0).numpy().astype(\"uint64\"), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(title)\n",
    "    plt.close()\n",
    "show_images2(tens[0].view(C, H, W).detach().cpu(), n=1, title='true.png')\n",
    "show_images2(tens[3].view(C, H, W).cpu(), n=1, title='middle.png')\n",
    "show_images2(tens[-1].view(C, H, W).detach().cpu(), n=1, title='noisy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.01 # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "\n",
    "# model = UniformRate(cfg, 'cuda')\n",
    "model_2 = UniformRate(cfg, \"cuda\")\n",
    "S = 256\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 8)\n",
    "tensor_list = []\n",
    "\n",
    "for i in ls:\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    # ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * i\n",
    "    ts = torch.clamp(ts, max=0.99999)\n",
    "    # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "    print(\"Time points\", ts[:9])\n",
    "    qt0 = model_2.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model_2.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "    print(qt0_rows_reg)  # , qt0)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    # x_tilde = x_tilde.view(B, C, H, W)\n",
    "    x_tilde = x_tilde.view(C, H, W)\n",
    "    tensor_list.append(x_tilde)\n",
    "    # show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=B) # * 127.5\n",
    "    # show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "tens = torch.stack(tensor_list, 0).to(device)\n",
    "tens[0] = minibatch.view(C, H, W)\n",
    "show_images(tens.detach().cpu(), n=len(ls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(tens[0].detach().cpu(), n=1, title='true.png')\n",
    "show_images(tens[3].detach().cpu(), n=1, title='middle.png')\n",
    "show_images(tens[-1].detach().cpu(), n=1, title='noisy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_tauUnet_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "model = model_utils.create_model(cfg, cfg.device)\n",
    "print(\"Number of Parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.networks.ddsm_networks import ProteinScoreNet\n",
    "from config.maze_config.config_hollow_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "cfg.device = \"cuda\"\n",
    "\n",
    "\n",
    "D = cfg.model.concat_dim = 1024\n",
    "S = cfg.data.S = 5\n",
    "B = cfg.data.batch_size\n",
    "net = ProteinScoreNet(cfg).to(cfg.device)\n",
    "ts = torch.rand((B,), device=cfg.device) * (1.0 - 0.01) + 0.01\n",
    "x = torch.randint(low=0, high=S, size=(B, D), device=\"cuda\")\n",
    "x_out = net(x, ts)\n",
    "print(x_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.mnist_config.config_hollow_mnist import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "#cfg.data.name = 'DiscreteCIFAR10'\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, cfg.data.location)\n",
    "cfg.data.batch_size = 16\n",
    "mnist_dl = DataLoader(\n",
    "    mnist_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)\n",
    "#cfg.model.time_base = 3\n",
    "#cfg.model.time_exp = 100\n",
    "#cfg.model.rate_sigma = 6.0\n",
    "#cfg.model.Q_sigma = 512.0\n",
    "\n",
    "#model = GaussianTargetRate(cfg, device)\n",
    "n_samples = cfg.data.batch_size\n",
    "for samples in mnist_dl:\n",
    "    print(samples.shape)\n",
    "    samples = samples.cpu().numpy()\n",
    "    samples = samples.reshape(n_samples, 1, cfg.data.image_size, cfg.data.image_size)\n",
    "    #saving_train_path = os.path.join(cfg.saving.sample_plot_path, f\"{cfg.model.name}{state['n_iter']}_{cfg.sampler.name}{cfg.sampler.num_steps}.png\")\n",
    "    fig = plt.figure(figsize=(9, 9)) \n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(int(np.sqrt(n_samples)), int(np.sqrt(n_samples)), 1 + i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(samples[i, ...], (1,2,0)), cmap=\"gray\")\n",
    " \n",
    " \n",
    "    plt.savefig('mnist_data.png', bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.synthetic_config.config_hollow_synthetic import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "location = \"lib/datasets/Synthetic/data_2spirals.npy\"\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "synthetic_dataset = dataset_utils.get_dataset(cfg, device, location)\n",
    "cfg.data.batch_size = 1500\n",
    "synthetic_dl = DataLoader(\n",
    "    synthetic_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 50000000 # 0.00009\n",
    "cfg.model.t_func = \"sqrt_cos\"\n",
    "#model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 2\n",
    "bm, inv_bm = synthetic.get_binmap(cfg.model.concat_dim, cfg.data.binmode)\n",
    "ls = np.linspace(0.001, 1, 10)\n",
    "for minibatch in synthetic_dl:\n",
    "#for i in ls:\n",
    "    # print(minibatch.device)\n",
    "    B, D = minibatch.shape\n",
    "    minibatch = minibatch.view(B, D).to(\"cuda\")\n",
    "    ts = torch.ones((B,), device=device) * 0.0000000000000001 #0.00001\n",
    "\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "    print(qt0)\n",
    "    log_qt0 = torch.where(qt0_rows_reg <= 0.0, -1e9, torch.log(qt0_rows_reg))\n",
    "    #print(log_qt0)\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(logits=log_qt0)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    \n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "    print(rate_new_val_probs)\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = synthetic.bin2float(\n",
    "        x_tilde.detach().cpu().numpy().astype(np.int32),\n",
    "        inv_bm,\n",
    "        cfg.model.concat_dim,\n",
    "        cfg.data.int_scale,\n",
    "    )\n",
    "\n",
    "    minibatch = synthetic.bin2float(minibatch.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    #x_tilde = synthetic.bin2float(x_tilde.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    #plot_samples(minibatch) # * 127.5\n",
    "    plot_samples(x_tilde) # * 127.5\n",
    "    #noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "\n",
    "    # noise_x = synthetic.bin2float(noise_x.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    # plot_samples(noise_x) # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.1  # 0.00009\n",
    "cfg.model.t_func = \"log\"\n",
    "\n",
    "cfg.model.time_base = 3\n",
    "cfg.model.time_exp = 100\n",
    "cfg.model.rate_sigma = 6.0\n",
    "cfg.model.Q_sigma = 512.0\n",
    "# model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 256\n",
    "\n",
    "\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    print(minibatch.device)\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D).to(\"cpu\")\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * 1\n",
    "    print(\"Time points\", ts[:9])\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "    print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(rate_vals_square_dimsum)\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(rate_new_val_probs)\n",
    "\n",
    "    square_newval_samples = square_newvalcat.sample()  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu(), n=9)  # * 127.5\n",
    "    show_images(x_tilde.detach().cpu(), n=9)  # * 127.5\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W).detach().cpu(), n=9)  # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_hollow_maze import get_config\n",
    "\n",
    "cfg = get_config()\n",
    "device = cfg.device\n",
    "device = \"cuda\"\n",
    "maze_dataset = dataset_utils.get_dataset(cfg, device)\n",
    "cfg.data.batch_size = 16\n",
    "maze_dl = DataLoader(\n",
    "    maze_dataset, cfg.data.batch_size, shuffle=cfg.data.shuffle, num_workers=0\n",
    ")\n",
    "# worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 1.7  # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619 [0.3511, 0.3245, 0.3245], [0.3444, 0.3278, 0.3278], [0.4763, 0.2619, 0.2619], 0.5557, 0.2221, 0.2221],\n",
    "cfg.model.t_func = \"sqrt_cos\"  # sqrt_cos\n",
    "cfg.model.time_base = 1\n",
    "cfg.model.time_exp = 250\n",
    "#model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "S = 3\n",
    "# 0.25 [0.7867, 0.1116, 0.1116]\n",
    "# 0.5 [0.4763, 0.2619, 0.2619\n",
    "# 1 [0.3389, 0.3305, 0.3305],\n",
    "ls = np.linspace(0.001, 1, 10)\n",
    "tensor_list = []\n",
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    D = C * H * W\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    # ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * 0.9999 #0.99#0.0001\n",
    "    # ts = torch.linspace(0.01, 1, B, device=device)\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "    print(qt0_rows_reg)  # , qt0)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    log_qt0 = torch.where(qt0_rows_reg <= 0.0, -1e9, torch.log(qt0_rows_reg))\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(logits=log_qt0)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "    #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    # x_tilde = x_tilde.view(B, C, H, W)\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    tensor_list.append(x_tilde)\n",
    "    np.save('mazes.npy', minibatch.cpu().numpy())\n",
    "    #show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=B) # * 127.5\n",
    "    #show_images(x_tilde.detach().cpu() * 127.5, n=B, title='mazes_noisy2.png') # * 127.5\n",
    "    # noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    # show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = minibatch.cpu().numpy().reshape(cfg.data.batch_size , 1, cfg.data.image_size, cfg.data.image_size)\n",
    "#saving_train_path = os.path.join(cfg.saving.sample_plot_path, f\"{cfg.model.name}{state['n_iter']}_{cfg.sampler.name}{cfg.sampler.num_steps}.png\")\n",
    "#fig = plt.figure(figsize=(9, 9)) \n",
    "for i in range(cfg.data.batch_size):\n",
    "    plt.subplot(int(np.sqrt(cfg.data.batch_size )), int(np.sqrt(cfg.data.batch_size)), 1 + i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(samples[i, ...], (1,2,0)), cmap=\"gray\")\n",
    "\n",
    "\n",
    "#plt.savefig(saving_train_path)\n",
    "    \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu() * 127.5, n=8)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.utils.utils as utils\n",
    "\n",
    "cfg.model.rate_const = 0.5\n",
    "cfg.model.t_func = \"log\"\n",
    "cfg.model.time_base = 5\n",
    "cfg.model.time_exp = 5\n",
    "model = UniformRate(cfg, \"cuda\")\n",
    "model = UniformVariantRate(cfg, \"cuda\")\n",
    "device = \"cpu\"\n",
    "S = 3\n",
    "min_time = 0.01\n",
    "\n",
    "\n",
    "for minibatch in mnist_dataset:\n",
    "    if len(minibatch.shape) == 4:\n",
    "        B, C, H, W = minibatch.shape\n",
    "        minibatch = minibatch.view(B, C * H * W)\n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    B = minibatch.shape[0]\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - min_time) + min_time\n",
    "    ts = torch.ones((B,)) * 1\n",
    "    print(ts[:9])\n",
    "    #\n",
    "\n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(\n",
    "        torch.arange(B, device=device), (tuple(range(1, minibatch.dim())))\n",
    "    )\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    x_tilde = torch.distributions.categorical.Categorical(\n",
    "        logits=log_qt0\n",
    "    ).sample()  # bis hierhin <1 sek\n",
    "\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    print(torch.mean(x_tilde[1, :, :, :].float()))\n",
    "    # print(x_tilde[0,0, :, :].std())\n",
    "    show_images(minibatch.view(B, C, H, W) * 127.5, n=9)\n",
    "    show_images(x_tilde * 127.5, n=9)\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W) * 127.5, n=9)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int log(t**2 + 1)\n",
    "t = np.linspace(0.01, 1, 1000)\n",
    "f = 2 * t / (t**2 + 1)\n",
    "f_int = np.log(t**2 + 1)\n",
    "f_cos = np.sin(t) / np.sqrt(np.cos(t))\n",
    "a = 5\n",
    "b = 5\n",
    "f_log = a * np.log(b) * b**t\n",
    "plt.plot(f, label=\"log sqr\")\n",
    "plt.plot(f_int, label=\"int log sqr\")\n",
    "plt.plot(f_cos, label=\"cos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
